{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('reddit_train.csv')\n",
    "dataTest = pd.read_csv('reddit_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store subreddit names and numbers\n",
    "subRedditNumber = {}\n",
    "\n",
    "j = 0\n",
    "\n",
    "y = data['subreddits'].tolist()\n",
    "\n",
    "for i in range(0, len(y)):\n",
    "    if y[i] in subRedditNumber.values():\n",
    "        continue\n",
    "    else:\n",
    "        subRedditNumber[j] = y[i]\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "comments_total = data['comments'].tolist() + dataTest['comments'].tolist()\n",
    "\n",
    "#Vectorizing\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(comments_total)\n",
    "#Reddit Test for Kaggle\n",
    "TestX = X[70000:len(comments_total)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5788571428571428\n",
      "0.5687142857142857\n",
      "0.5632857142857143\n",
      "0.5714285714285714\n",
      "0.5817142857142857\n",
      "0.5771428571428572\n",
      "0.5771428571428572\n",
      "0.5701428571428572\n",
      "0.57\n",
      "0.5782857142857143\n",
      "0.5752857142857143\n",
      "0.573\n",
      "0.5695714285714286\n",
      "0.5752857142857143\n",
      "0.5724285714285714\n",
      "0.5684285714285714\n",
      "0.5701428571428572\n",
      "0.5748571428571428\n",
      "0.5655714285714286\n",
      "0.5728571428571428\n",
      "0.5708571428571428\n",
      "0.5684285714285714\n",
      "0.5632857142857143\n",
      "0.5707142857142857\n",
      "0.5667142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#Implementing Bagging\n",
    "\n",
    "allpredictions = []\n",
    "\n",
    "for i in range(25):\n",
    "    X_train_loop, X_test_loop, Y_train_loop, Y_test_loop = train_test_split(X[:70000], y, test_size=0.1)\n",
    "    #Normalizing\n",
    "    X_train_loop = normalize(X_train_loop)\n",
    "    X_test_loop = normalize(X_test_loop)\n",
    "    clf_nb_loop = MultinomialNB(alpha=0.2).fit(X_train_loop, Y_train_loop)\n",
    "    #Validation Test scores\n",
    "    score_nb = clf_nb_loop.score(X_test_loop, Y_test_loop)\n",
    "    predictions = clf_nb_loop.predict(TestX)\n",
    "    allpredictions.append(predictions)\n",
    "    print (score_nb)\n",
    "\n",
    "votingPredictions = []\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(30000):\n",
    "    c = Counter([allpredictions[0][i], allpredictions[1][i], allpredictions[2][i], allpredictions[3][i], \n",
    "                allpredictions[4][i], allpredictions[5][i], allpredictions[6][i], allpredictions[7][i], \n",
    "                allpredictions[8][i], allpredictions[9][i], allpredictions[10][i], allpredictions[11][i], \n",
    "                allpredictions[12][i], allpredictions[13][i], allpredictions[14][i], allpredictions[15][i],\n",
    "                allpredictions[16][i], allpredictions[17][i], allpredictions[18][i], allpredictions[19][i], \n",
    "                allpredictions[20][i], allpredictions[21][i], allpredictions[22][i], allpredictions[23][i], \n",
    "                allpredictions[24][i]])\n",
    "    value, count = c.most_common()[0]\n",
    "    votingPredictions.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script for submission to Kaggle\n",
    "\n",
    "Category = []\n",
    "\n",
    "for i in range(0,len(votingPredictions)):\n",
    "    for key, val in subRedditNumber.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "        if val == votingPredictions[i]:\n",
    "            Category.append(val)\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "Id = []\n",
    "\n",
    "for i in range(0, len(votingPredictions)):\n",
    "    Id.append(i)\n",
    "    \n",
    "Cars = {'Id': Id, 'Category': Category}\n",
    "\n",
    "submissionDF = DataFrame(Cars, columns= ['Id', 'Category'])\n",
    "\n",
    "submissionDF.head()\n",
    "\n",
    "submissionDF.to_csv('submission_19_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
